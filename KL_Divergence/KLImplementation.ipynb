{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## KL Divergence Score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1 Take the simple sentences and compute the K-L divergence scores for them, in both directions. Now create a third story that is very different to the other two, add it to the program and report how its score changes relative to the first two. Explain what role epsilon and gamma play in the computation of K-L."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "d1 =  john fell down harry fell as-well down stream sun shone before down marry fine\n",
      "d2 =  bill fell down jeff fell too down river sun shone until down belinda ill\n",
      "d3 =  It is expensive and the battery does not last as long as I would like but that has not stopped me from thoroughly enjoying my time with the Pixel 4 XL\n",
      "\n",
      "\n",
      "d1 after tokenizing:   defaultdict(<class 'int'>, {'john': 1, 'fell': 2, 'down': 3, 'harry': 1, 'well': 1, 'stream': 1, 'sun': 1, 'shone': 1, 'before': 1, 'marry': 1, 'fine': 1})\n",
      "\n",
      "d2 after tokenizing:   defaultdict(<class 'int'>, {'bill': 1, 'fell': 2, 'down': 3, 'jeff': 1, 'river': 1, 'sun': 1, 'shone': 1, 'until': 1, 'belinda': 1, 'ill': 1})\n",
      "\n",
      "d3 after tokenizing:   defaultdict(<class 'int'>, {'expensive': 1, 'battery': 1, 'does': 1, 'not': 2, 'last': 1, 'long': 1, 'would': 1, 'like': 1, 'but': 1, 'has': 1, 'stopped': 1, 'me': 1, 'from': 1, 'thoroughly': 1, 'enjoying': 1, 'my': 1, 'time': 1, 'with': 1, 'pixel': 1, 'xl': 1})\n",
      "\n",
      "\n",
      "KL-divergence between d1 and d2: 3.4532350153970492\n",
      "KL-divergence between d2 and d1: 3.2222889623006754\n",
      "KL-divergence between d1 and d3: 7.643742275516928\n",
      "KL-divergence between d2 and d3: 7.743889202503388\n"
     ]
    }
   ],
   "source": [
    "# Manos Tsagkias' program for computing Kullback-Liebler Divergence\n",
    "# Using the Migge (2003) smoothening backoff\n",
    "# see http://staff.science.uva.nl/~tsagias/?s=kullback\n",
    "# updated for Python3 by Mark Keane 30-June-2014\n",
    "\n",
    "import re, math, collections\n",
    "from collections import defaultdict, deque\n",
    "\n",
    "def tokenize(_str):\n",
    "\n",
    "    stopwords = ['and', 'for', 'if', 'too', 'as', 'the', 'then', 'be', 'is', 'are', 'will', 'in', 'it', 'to', 'that', '-']\n",
    "    tokens = collections.defaultdict(int)\n",
    "    for m in re.finditer(r\"(\\w+)\", _str, re.UNICODE):\n",
    "        m = m.group(1).lower()\n",
    "        if len(m) < 2: continue\n",
    "        if m in stopwords: continue\n",
    "        tokens[m] += 1\n",
    "    return tokens\n",
    "#end of tokenize\n",
    "\n",
    "def kldiv(_s, _t):\n",
    "    if (len(_s) == 0):\n",
    "        return 1e33\n",
    "    if (len(_t) == 0):\n",
    "        return 1e33\n",
    "    ssum = 0. + sum(_s.values())\n",
    "    slen = len(_s)\n",
    "    \n",
    "    tsum = 0. + sum(_t.values())\n",
    "    tlen = len(_t)\n",
    "    vocabdiff = set(_s.keys()).difference(set(_t.keys()))\n",
    "    #print(vocabdiff)\n",
    "    lenvocabdiff = len(vocabdiff)\n",
    "    \n",
    "    #print(\"_s: %s\" % _s)\n",
    "    #print(\"_t: %s\" % _t)\n",
    "    #print(\"%s\" % vocabdiff)\n",
    "\n",
    "    \"\"\" epsilon \"\"\"\n",
    "    epsilon = min(min(_s.values())/ssum, min(_t.values())/tsum) * 0.001\n",
    "    #print('epsilon: ',epsilon)\n",
    "    \n",
    "    \"\"\" gamma \"\"\"\n",
    "    gamma = 1 - lenvocabdiff * epsilon\n",
    "    #print('gamma:',gamma)\n",
    "    \n",
    "    \"\"\" Check if distribution probabilities sum to 1\"\"\"\n",
    "    sc = sum([v/ssum for v in _s.values()])  \n",
    "    st = sum([v/tsum for v in _t.values()]) \n",
    "    \n",
    "    if sc < 9e-6:\n",
    "        print(\"Sum P: %e, Sum Q: %e\" % (sc, st))\n",
    "        print(\"*** ERROR: sc does not sum up to 1. Bailing out ..\")\n",
    "        sys.exit(2)\n",
    "    if st < 9e-6:\n",
    "        print(\"Sum P: %e, Sum Q: %e\" % (sc, st))\n",
    "        print(\"*** ERROR: st does not sum up to 1. Bailing out ..\")\n",
    "        sys .exit(2)\n",
    "\n",
    "    div = 0.\n",
    "    for t, v in _s.items(): \n",
    "        pts = v / ssum\n",
    "        ptt = epsilon\n",
    "        if t in _t:\n",
    "            ptt = gamma * (_t[t] / tsum)\n",
    "            \n",
    "        ckl = (pts - ptt) * math.log(pts / ptt)\n",
    "\n",
    "        div +=  ckl\n",
    "    return div\n",
    "\n",
    "#end of kldiv\n",
    "\n",
    "d1 = \"\"\"john fell down harry fell as-well down stream sun shone before down marry fine\"\"\"\n",
    "\n",
    "d2 = \"\"\"bill fell down jeff fell too down river sun shone until down belinda ill\"\"\"\n",
    "\n",
    "d3 = \"\"\"It is expensive and the battery does not last as long as I would like but that has not stopped me from thoroughly enjoying my time with the Pixel 4 XL\"\"\"\n",
    "\n",
    "\n",
    "print('d1 = ',d1)\n",
    "print('d2 = ',d2)\n",
    "print('d3 = ',d3)\n",
    "\n",
    "\n",
    "print('\\n\\nd1 after tokenizing:  ',tokenize(d1))\n",
    "print('\\nd2 after tokenizing:  ',tokenize(d2))\n",
    "print('\\nd3 after tokenizing:  ',tokenize(d3))\n",
    "\n",
    "\n",
    "\n",
    "print(\"\\n\\nKL-divergence between d1 and d2:\", kldiv(tokenize(d1), tokenize(d2)))\n",
    "print(\"KL-divergence between d2 and d1:\", kldiv(tokenize(d2), tokenize(d1)))\n",
    "print(\"KL-divergence between d1 and d3:\", kldiv(tokenize(d1), tokenize(d3)))\n",
    "print(\"KL-divergence between d2 and d3:\", kldiv(tokenize(d2), tokenize(d3)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
